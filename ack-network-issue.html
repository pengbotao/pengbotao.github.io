<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no">
    <title>阿里云ACK集群中的网络问题</title>
    <link rel="stylesheet" href="/static/css/markdown.css">
    
</head>
<body>
<div class="content">
    <h1 class="title">阿里云ACK集群中的网络问题</h1>

    <a href="/"><img src="/static/img/arrow-back.png" class="title_arrow_back" /></a>

    <p>安装的ACK集群已经解决了VPC内的网络通信，安装好的k8s集群网络概况如下：</p>

<ul>
<li>每个pod都有独立的IP地址，不同pod可以监听相同的端口，该规则也适用svc</li>
<li>同vpc内k8s集群与非k8s集群之间ECS网络是互通的</li>
<li>k8s集群内pod内的网络的互通的</li>
<li>k8s集群内pod可以访问vpc内的所有ECS，但集群外的ECS无法访问到pod，集群内的ECS可以正常访问pod</li>
</ul>

<p>但向容器迁移时还是会有一些兼容性的问题，比如有一些内部服务先迁移到容器，k8s集群外vpc内的项目需要访问到；有一些服务有端口，要考虑兼容性的话需要按指定端口暴露出去。针对这些整理一下服务迁移到k8s集群可能面对的网络兼容问题。</p>

<h1 id="1-pod无法访问自己暴露出去的服务">1. Pod无法访问自己暴露出去的服务</h1>

<p><strong>问题说明：</strong></p>

<blockquote>
<p>正常创建Pod和Service，Service以默认方式生成ClusterIP，在容器内部访问该ClusterIP时可能时好时坏，访问其他服务暴露的ClusterIP没有问题，但如果是自己对应的Service并且Endpoint调度到自己的时候就失败（注意是从容器内部访问）。</p>
</blockquote>

<p>过程如下，查看Pod对应列表</p>

<pre><code>$ kubectl get pod -o wide | grep k8s-go-demo
k8s-go-demo-deploy-66cdbcd8d9-fxrql           1/1     Running     0          28d     10.1.0.141   192.168.0.43
k8s-go-demo-deploy-66cdbcd8d9-x88b8           1/1     Running     0          28d     10.1.0.218   192.168.0.45
</code></pre>

<p>进入k8s-go-demo-deploy-66cdbcd8d9-fxrql，当前节点所在的机器IP是:192.168.0.43，访问当前服务的ClusterIP，可以看到访问者IP是10.1.0.141，服务的IP是10.1.0.218，他们处于不同的节点。</p>

<pre><code>$ curl 10.2.212.203
{
    &quot;ClientIP&quot;: &quot;10.1.0.141&quot;,
    &quot;Host&quot;: &quot;k8s-go-demo-deploy-66cdbcd8d9-x88b8&quot;,
    &quot;ServerIP&quot;: &quot;10.1.0.218&quot;,
    &quot;Time&quot;: &quot;2021-02-25 21:13:00&quot;,
    &quot;Version&quot;: &quot;latest&quot;
}
</code></pre>

<p>但多刷几次会发现好一次坏一次，不是每次都能成功，但每次拿到的ServerIP都是10.1.0.218。但倘若替换ClusterIP的服务为Headness方式则可以。</p>

<pre><code>apiVersion: v1
kind: Service
metadata:
  name: k8s-go-demo-svc-headless
  labels:
    project: k8s-go-demo-svc-headless
spec:
  selector:
    app: k8s-go-demo
  ports:
  - port: 80
    targetPort: 38001
    protocol: TCP
  clusterIP: None
</code></pre>

<p>还是进入前面的容器访问：</p>

<pre><code>/ # curl k8s-go-demo-svc-headless:38001
{
    &quot;ClientIP&quot;: &quot;10.1.0.141&quot;,
    &quot;Host&quot;: &quot;k8s-go-demo-deploy-66cdbcd8d9-x88b8&quot;,
    &quot;ServerIP&quot;: &quot;10.1.0.218&quot;,
    &quot;Time&quot;: &quot;2021-02-25 21:21:05&quot;,
    &quot;Version&quot;: &quot;latest&quot;
}

/ # curl k8s-go-demo-svc-headless:38001
{
    &quot;ClientIP&quot;: &quot;10.1.0.141&quot;,
    &quot;Host&quot;: &quot;k8s-go-demo-deploy-66cdbcd8d9-fxrql&quot;,
    &quot;ServerIP&quot;: &quot;10.1.0.141&quot;,
    &quot;Time&quot;: &quot;2021-02-25 21:21:07&quot;,
    &quot;Version&quot;: &quot;latest&quot;
}
</code></pre>

<p><strong>原因说明：Flannel目前默认设置不允许回环访问</strong></p>

<p><img src="../../static/uploads/k8s-flannel-issue.png" alt="" /></p>

<p>参考：<a href="https://help.aliyun.com/knowledge_detail/197320.html">容器网络FAQ</a></p>

<h1 id="2-coredns定义域名解析">2. CoreDNS定义域名解析</h1>

<p>可能需要针对性的对某些域名配置hosts，常见的比如内部服务绑定SLBIP，内部在通过配置hosts的方式定义好域名，程序就可以直接通过域名来访问了。K8s集群内的DNS解析使用的是CoreDNS，可以通过修改他的configmap来实现对某些域名的解析：</p>

<pre><code>$ kubectl get configmap coredns -n kube-system -o yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: coredns
  namespace: kube-system
data:
  Corefile: |
    .:53 {
        errors
        health {
           lameduck 5s
        }
        ready

        kubernetes cluster.local in-addr.arpa ip6.arpa {

          pods verified
          upstream
          fallthrough in-addr.arpa ip6.arpa
          ttl 30
        }
        autopath @kubernetes
        prometheus :9153
        forward . /etc/resolv.conf
        cache 30
        loop
        reload
        loadbalance
    }
</code></pre>

<p>添加Hosts配置，保存后稍等就会生效。这样子容器内部就可以直接访问demo.api.com了。</p>

<pre><code>$ kubectl edit configmap coredns -n kube-system
        autopath @kubernetes
        hosts {
            192.168.1.100     demo.api.com
            192.168.1.200     uat.api.com
            fallthrough
        }
        prometheus :9153
</code></pre>

<p>除了配置Hosts也可以通过配置CNAME的方式调整解析：</p>

<pre><code>rewrite stop {
  name regex demo.api.com demo.default.svc.cluster.local
  answer name demo.default.svc.cluster.local demo.api.com
}
</code></pre>

<h1 id="3-创建多套ingress">3. 创建多套Ingress</h1>

<p><strong>场景说明：</strong></p>

<blockquote>
<p>迁移项目到Kubernetes集群，但无法一步到位，新建k8s集群和原有机器节点在同一个vpc下。那迁移到集群的项目有内网项目和公网项目，而初始化集群创建的SLB如果是私网，那公网项目的Ingress就无法对外（vpc外部）；如果是公网，则内部服务通过Ingress就暴露出去了，只是可能没有在DNS中指定Ingress对应的域名解析，但实际绑hosts到公网IP后内网服务就暴露了。</p>
</blockquote>

<p>vpc网络内kubernetes集群外的节点对容器来说也是外部访问，可以通过Ingress来暴露，但从业务角度来说，认为它还是内部服务，不能对vpc之外。所以如果有两套SLB就可以解决了，一套针对的是vpc内部，一套针对的是公网。</p>

<p>创建方式可以参考：<a href="https://help.aliyun.com/document_detail/151524.html">部署多个Ingress Controller</a></p>

<p>需要注意的<strong>ingressClass</strong>，用来确定Ingress表示，会在Ingress的Yaml文件中关联。注意的是这个组件会自动创建SLB，不需要手动指定，后续想要对SLB扩容直接在SLB控制台上操作即可，修改或删除Worker节点也不需要手动修改SLB监听和配置。</p>

<p>后续Ingress想要绑定到新建的SLB上，可以如下配置：</p>

<pre><code>apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: nginx
  annotations:
    #注意这里要设置为您前面配置的INGRESS_CLASS。
    kubernetes.io/ingress.class: &quot;&lt;YOUR_INGRESS_CLASS&gt;&quot;
spec:
  rules:
  - host: foo.bar.com
    http:
      paths:
      - path: /
        backend:
          serviceName: nginx
          servicePort: 80
</code></pre>

<h1 id="4-通过slb来暴露服务">4. 通过SLB来暴露服务</h1>

<p>Service有几种类型，默认是ClusterIP，只能集群内部访问；还有一种是LoadBalancer，在阿里云ACK内，如果Service的类型设置为Type=LoadBalancer，则容器服务ACK的CCM(Cloud Controller Manager)组件会为该Service创建或配置阿里云的SLB。</p>

<p>前一步中已经通过Ingress实现了内外网的SLB分离，但还是有个问题，如果原有服务带非80端口，则Ingress是不方便处理的。这个时候我们可以通过LoadBalancer来暴露服务。</p>

<p><strong>第一步，</strong> 手动创建一个SLB，也可以不设置自动创建，问题是自动创建出来的slb不支持复用，关于区别可以看下面参考文档。</p>

<p><strong>第二步，</strong> 创建Service并指定类型为LoadBalancer</p>

<pre><code>apiVersion: v1
kind: Service
metadata:
  name: k8s-go-demo-slb
  annotations:
    service.beta.kubernetes.io/alibaba-cloud-loadbalancer-id: ${YOUR_LB_ID}
    service.beta.kubernetes.io/alicloud-loadbalancer-force-override-listeners: 'true'
  labels:
    project: k8s-go-demo-slb
spec:
  selector:
    app: k8s-go-demo
  ports:
  - port: 38001
    targetPort: 38001
    protocol: TCP
  type: LoadBalancer
</code></pre>

<p>该操作会自动创建SLB的端口监听、虚拟服务器组，然后通过SLB的IP加服务端口在外部就可以访问了。</p>

<p>参考文档：</p>

<ul>
<li><a href="https://help.aliyun.com/document_detail/181518.html">通过使用已有SLB的服务公开应用</a></li>
<li><a href="https://help.aliyun.com/document_detail/181517.html">Service的负载均衡配置注意事项</a></li>
<li><a href="https://help.aliyun.com/document_detail/86531.html">通过Annotation配置负载均衡</a></li>
</ul>

<h1 id="5-pod内访问slb不通">5. Pod内访问SLB不通</h1>

<p>Service中有一个参数spec.externalTrafficPolicy，可定义为Local或者Cluster，默认为Local。这种类型的SLB地址只有在Node中部署了对应的后端Pod才能被访问，所以前面的Ingress、Service通过SLB暴露服务时需要注意下内部访问情况。</p>

<pre><code>$ kubectl get svc nginx-ingress-lb -n kube-system -o yaml | grep externalTrafficPolicy

  externalTrafficPolicy: Local
</code></pre>

<p>也就是Local模式下，只能从Service的Endpoints对应的Node或者Node里的Pod访问到，其他就会出现网络不通的情况。SLB本身也是提供集群外访问，不是集群内访问。可以通过修改为Cluster但<strong>会丢失源IP</strong>，其他方法见下面参考文档。</p>

<p>参考文档：<a href="https://help.aliyun.com/knowledge_detail/171437.html">Kubernetes集群中访问LoadBalancer暴露出去的SLB地址不通</a></p>

<p>而且这个方案还有一个问题是容易触发到SLB的上限： <a href="https://slbnext.console.aliyun.com/slb/quota?spm=5176.smartservice_service_chat.0.0.49373f1bbsfycD">https://slbnext.console.aliyun.com/slb/quota</a></p>

<p>一个SLB开M个端口，目前集群有N个节点，则一个SLB上面会有M*N个ECS实例，而单个SLB的服务器上限默认为200.如果你有50个Node，则每个SLB上只能开4个端口。但为啥还考虑调整为Cluster呢？还是因为旧的内部业务迁移到容器内后，容器内和容器外都还需要访问，但容器内有容器内的访问地址，如果迁移过程不想容器内的调用方调整访问地址，就需要考虑到这个问题。</p>

<hr />

<p>[1] <a href="https://help.aliyun.com/document_detail/86987.html">Kubernetes集群用户指南</a></p>

    <div class="eof">-- EOF --</div>
    <div class="eof_arrow">
        <a href="/"><img src="/static/img/arrow-back.png" style="width:25px;height:25px;" /></a>
    </div>
    
    <div class="eof_tag">
        最后更新于：
        <code style="border:0px;background:none;"><a href="/2021-02.html">2021-11-27 13:41</a></code>
    </div>
    
    <div class="eof_tag">
        发表于：
        <code style="border:0px;background:none;"><a href="/2021-02.html">2021-02-25 21:18</a></code>
    </div>
    <div class="eof_tag">
        标签：
        <code style="border:0px;background:none;"><a href="/tag/kubernetes.html">Kubernetes</a></code>
        <code style="border:0px;background:none;"><a href="/tag/容器化.html">容器化</a></code>
    </div>

    <div id="footer">
        <ul>
            <li>
            <b>上一篇</b>：<a href="/go-tips.html">Golang易错知识点</a>
            </li>
            
            <li>
            <b>下一篇</b>：<a href="/helm.html">Kubernetes包管理工具 - Helm</a>
            </li>
            <li>
                <b>Github地址</b>：<a href="https://github.com/pengbotao/itopic.go/blob/master/posts/kubernetes/阿里云ACK集群中的网络问题.md">https://github.com/pengbotao/itopic.go/blob/master/posts/kubernetes/阿里云ACK集群中的网络问题.md</a>
            <li>
            <li>
                @2013-2022 老彭的博客&nbsp;[Hosted by <a href="https://pages.github.com/" style="font-weight: bold" target="_blank">Github Pages</a>]
            </li>
        </ul>
    </div>
</div>
<div id="top"><a href="#"><img src="/static/img/arrow-top.png" style="width:40px;height:40px;" /></a></div>

<a href="https://github.com/pengbotao/itopic.go/blob/master/posts/kubernetes/阿里云ACK集群中的网络问题.md" target="_blank"  class="github-corner">
<svg width="60" height="60" viewBox="0 0 250 250" style="fill: #61687C; color:#fff; position: absolute;top: 0;border: 0;right: 0;" aria-hidden="true">
    <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
    <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
    <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path>
    </svg>
</a>
</body>
</html>
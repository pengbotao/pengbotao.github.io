<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no">
    <title>CentOS（ 5 ）- 内核参数优化</title>
    <link rel="stylesheet" href="/static/css/markdown.css">
    <style type="text/css">
    @media (min-width: 1200px) {
        .content{padding-left:280px;width:100%;max-width:100%;}
    }
    </style>
</head>
<body>
<div class="content">
    <h1 class="title">CentOS（ 5 ）- 内核参数优化</h1>

    <a href="/"><img src="/static/img/arrow-back.png" class="title_arrow_back" /></a>

    <nav>
<ul>
<li><a href="#一-文件句柄数限制">一、文件句柄数限制</a></li>
<li><a href="#二-内核参数优化">二、内核参数优化</a>
<ul>
<li><a href="#2-1-tcp参数优化">2.1 TCP参数优化</a>
<ul>
<li><a href="#net-ipv4-tcp-syncookies">net.ipv4.tcp_syncookies</a></li>
<li><a href="#net-ipv4-tcp-tw-recycle">net.ipv4.tcp_tw_recycle</a></li>
<li><a href="#net-ipv4-tcp-tw-reuse">net.ipv4.tcp_tw_reuse</a></li>
<li><a href="#net-ipv4-tcp-fin-timeout">net.ipv4.tcp_fin_timeout</a></li>
<li><a href="#net-ipv4-tcp-keepalive-time">net.ipv4.tcp_keepalive_time</a></li>
<li><a href="#net-ipv4-ip-local-reserved-ports">net.ipv4.ip_local_reserved_ports</a></li>
</ul></li>
<li><a href="#2-2-系统丢包">2.2 系统丢包</a>
<ul>
<li><a href="#net-netfilter-nf-conntrack-max">net.netfilter.nf_conntrack_max</a></li>
</ul></li>
<li><a href="#2-3-swap分区">2.3 Swap分区</a></li>
<li><a href="#2-4-内存分配策略">2.4 内存分配策略</a></li>
</ul></li>
<li><a href="#完整示例">完整示例</a></li>
</ul>
</nav>

<h1 id="一-文件句柄数限制">一、文件句柄数限制</h1>

<p><code>Linux</code>系统对单用户<code>单个进程</code>可打开的文件数量有限制，每一个<code>TCP</code>连接都需要创建一个<code>Socket</code>句柄，每个<code>Socket</code>句柄同时也是一个文件句柄，这个设置会直接影响到系统可支撑的最大连接数。</p>

<p>默认单个进程最大可打开<code>1024</code>个文件，通过<code>ulimit -n</code>可以查看打开的最大文件数限制：</p>

<pre><code>$ ulimit -n
1024
</code></pre>

<p>限制分为软限制和硬限制，<code>ulimit -n</code>显示的是软限制的数量，用户可以设置自己的限制数量，但受限于最大硬限制设置的值。</p>

<pre><code>[peng@peng-master-1 ~]$ ulimit -SHn 1000
[peng@peng-master-1 ~]$ ulimit -SHn 10000
-bash: ulimit: open files: 无法修改 limit 值: 不允许的操作
</code></pre>

<blockquote>
<p>注：ulimit命令只影响当前Shell环境</p>
</blockquote>

<p>如果要永久生效，可以通过调整文件<code>/etc/security/limits.conf</code></p>

<pre><code>$ cat /etc/security/limits.conf
* soft nofile 65536
* hard nofile 65536
</code></pre>

<p>其中类型的值可以为<code>hard</code>，<code>soft</code>或者<code>-</code>，其中<code>-</code>相当于同时配置了<code>hard</code>和<code>soft</code>两行。</p>

<p>软限制(<code>soft nofile</code>)是指<code>Linux</code>在当前系统能够承受的范围内进一步限制用户同时打开的文件数；硬限制(<code>hard nofile</code>)则是根据系统硬件资源状况(主要是系统内存)计算出来的系统最多可同时打开的文件数量。通常软限制小于或等于硬限制。</p>

<p>修改后重新登录即可生效。同时系统层面也有可打开的文件数量限制，根据系统的资源情况计算出来的。</p>

<p><strong>系统层面限制</strong></p>

<p>查看与临时修改系统打开文件总数限制：</p>

<pre><code>$ cat /proc/sys/fs/file-max
95079
$ echo 1620826 &gt; /proc/sys/fs/file-max
</code></pre>

<p>永久修改系统打开文件最大值限制：</p>

<pre><code>$ sysctl -a | grep file-max
fs.file-max = 1620826

$ vi /etc/sysctl.conf
fs.file-max = 1000000

# 立即生效：
$ sysctl -p
</code></pre>

<p><strong>查看进程资源限制与打开的文件</strong></p>

<pre><code>$ cat /proc/2309/limits
Limit                     Soft Limit           Hard Limit           Units
Max cpu time              unlimited            unlimited            seconds
Max file size             unlimited            unlimited            bytes
Max data size             unlimited            unlimited            bytes
Max stack size            10485760             unlimited            bytes
Max core file size        0                    unlimited            bytes
Max resident set          unlimited            unlimited            bytes
Max processes             63691                63691                processes
Max open files            1024                 4096                 files
Max locked memory         65536                65536                bytes
Max address space         unlimited            unlimited            bytes
Max file locks            unlimited            unlimited            locks
Max pending signals       63691                63691                signals
Max msgqueue size         819200               819200               bytes
Max nice priority         0                    0
Max realtime priority     0                    0
Max realtime timeout      unlimited            unlimited            us

$ lsof -p 2309
COMMAND    PID   USER   FD   TYPE             DEVICE SIZE/OFF    NODE NAME
zabbix_se 2309 zabbix  cwd    DIR              252,1     4096       2 /
zabbix_se 2309 zabbix  rtd    DIR              252,1     4096       2 /
zabbix_se 2309 zabbix  txt    REG              252,1  3506157 1050689 /usr/local/zabbix/sbin/zabbix_server
zabbix_se 2309 zabbix  mem    REG              252,1   803410  267760 /lib64/ld-2.18.so
zabbix_se 2309 zabbix  mem    REG              252,1 10114254  269365 /lib64/libc-2.18.so
zabbix_se 2309 zabbix  mem    REG              252,1   858324  269378 /lib64/libpthread-2.18.so
zabbix_se 2309 zabbix  mem    REG              252,1   102750  269377 /lib64/libdl-2.18.so
zabbix_se 2309 zabbix  mem    REG              252,1  2452986  269371 /lib64/libm-2.18.so
zabbix_se 2309 zabbix  mem    REG              252,1    91096  269369 /lib64/libz.so.1.2.3
zabbix_se 2309 zabbix  mem    REG              252,1   172659  269388 /lib64/librt-2.18.so
zabbix_se 2309 zabbix  mem    REG              252,1   340018  269509 /lib64/libresolv-2.18.so
zabbix_se 2309 zabbix  mem    REG              252,1   528075  269246 /lib64/libnsl-2.18.so
</code></pre>

<h1 id="二-内核参数优化">二、内核参数优化</h1>

<p><code>/etc/sysctl.conf</code>配置内核参数，修改之后生效需执行：<code>sysctl -p</code></p>

<h2 id="2-1-tcp参数优化">2.1 TCP参数优化</h2>

<p><img src="../../static/uploads/tcp-Three-Way-Handshake.png" alt="" /></p>

<p>（图一：Three-Way Handshake）</p>

<p><img src="../../static/uploads/tcp-Four-Way-Wavehand.png" alt="" /></p>

<p>（图二：Four-Way Wavehand）</p>

<h3 id="net-ipv4-tcp-syncookies">net.ipv4.tcp_syncookies</h3>

<p>net下的参数也可以在/proc/sys/net/下看到对应的文件。</p>

<pre><code>net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_synack_retries = 2
net.ipv4.tcp_max_syn_backlog = 8192
net.core.somaxconn = 65535
</code></pre>

<p>对于一个<code>TCP</code>连接，<code>Server</code>与<code>Client</code>需要通过三次握手来建立网络连接.当三次握手成功后,我们可以看到端口的状态由<code>LISTEN</code>转变为<code>ESTABLISHED</code>,接着这条链路上就可以开始传送数据了。</p>

<p>对于服务器而言，一个完整的连接建立过程，服务器会经历 2 种 TCP 状态：<code>SYN_REVD</code>, <code>ESTABELLISHED</code>。对应也会维护两个队列：</p>

<ol>
<li>一个存放<code>SYN</code>的队列（半连接队列）</li>
<li>一个存放已经完成连接的队列（全连接队列）</li>
</ol>

<p>当一个连接的状态是<code>SYN RECEIVED</code>时，它会被放在<code>SYN</code>队列中。当它的状态变为<code>ESTABLISHED</code>时，它会被转移到另一个队列。所以后端的应用程序只从已完成的连接的队列中获取请求。如果一个服务器要处理大量网络连接，且并发性比较高，那么这两个队列长度就非常重要了。因为，即使服务器的硬件配置非常高，服务器端程序性能很好，但是这两个队列非常小，那么经常会出现客户端连接不上的现象，因为这两个队列一旦满了后，很容易丢包，或者连接被复位。所以，如果服务器并发访问量非常高，那么这两个队列的设置就非常重要了。</p>

<p><code>net.ipv4.tcp_max_syn_backlog</code>：是指定所能接受<code>SYN</code>同步包的最大客户端数量，即半连接上限，默认值是128,即SYN_REVD状态的连接数。</p>

<p><code>net.core.somaxconn</code>：是Linux中的一个kernel参数，指的是服务端所能accept即处理数据的最大客户端数量，即完成连接上限，默认值是128.</p>

<p>如果客户端发出<code>SYNC</code>包后不回应<code>ACK</code>包，则可能造成半连接队列满。客户端可以用此发起<code>SYN FLOOD</code>攻击。而<code>SYN Cookie</code>技术可以让服务器在收到客户端的<code>SYN</code>报文时，不分配资源保存客户端信息，而是将这些信息保存在<code>SYN+ACK</code>的初始序号和时间戳中。对正常的连接，这些信息会随着<code>ACK</code>报文被带回来。上面配置开启了<code>net.ipv4.tcp_syncookies</code>参数。另外也可以配置：</p>

<ul>
<li><code>net.ipv4.tcp_synack_retries</code>：TCP失败重传次数，默认是15，可减少重传次数</li>
<li><code>net.ipv4.tcp_max_syn_backlo</code>：调整半连接队列大小</li>
</ul>

<h3 id="net-ipv4-tcp-tw-recycle">net.ipv4.tcp_tw_recycle</h3>

<p>查看当前系统的<code>TCP</code>连接情况，可以看到<code>TIME_WAIT</code>的数量比较多。在主动关闭的一方发出最后的<code>ACK</code>后状态变为<code>TIME_WAIT</code>，但这个时候会等待<code>2MSL</code>的时间，如果这个时间内没有收到被动关闭方的重传的<code>FIN</code>包，则结束<code>TCP</code>连接，主要是为了防止最后的<code>ACK</code>包发送后对方没有收到而重传<code>FIN</code>包。</p>

<p>但过多的<code>TIME_WAIT</code>会占用掉一部分端口，<code>TCP/IP</code>协议中端口号的范围是0-65535，也就是只有65536端口，而下面示例就占用了接近2万个。</p>

<pre><code>$ netstat -tn | awk '$1==&quot;tcp&quot;{print $NF}' | sort | uniq -c | sort -nr
  19622 TIME_WAIT
    166 ESTABLISHED
     12 SYN_SENT
      9 LAST_ACK
      8 FIN_WAIT2
      3 CLOSING
</code></pre>

<p><strong>这个接近2w的数值是怎么来的？</strong>从<code>sysctl.conf</code>中看到配置：</p>

<pre><code>net.ipv4.tcp_max_tw_buckets = 20000
</code></pre>

<p>这个参数表示系统保持<code>TIME_WAIT</code>的最大值，默认为180000。设为较小数值此项参数可以控制<code>TIME_WAIT</code>套接字的最大数量，避免服务器被大量的<code>TIME_WAIT</code>套接字拖死。</p>

<p>网上看到的优化<code>TIME_WAIT</code>方案基本是开启<code>net.ipv4.tcp_tw_recycle</code>，用于<code>TIME_WAIT</code>的快速回收。这个开关需要和<code>net.ipv4.tcp_timestamps</code>配合使用（默认开启），如：</p>

<pre><code>net.ipv4.tcp_tw_recycle = 1
net.ipv4.tcp_timestamps = 1
</code></pre>

<p>但貌似这个参数打开后问题比较多，所以目前线上系统并未开启这个配置。</p>

<ul>
<li><a href="https://blog.csdn.net/zhuyiquan/article/details/68925707">tcp_tw_recycle参数引发的系统问题</a></li>
<li><a href="https://witee.github.io/2019/01/24/%E8%B0%83%E6%95%B4net.ipv4.tcp_tw_recycle%E9%80%A0%E6%88%90%E7%9A%84%E6%95%85%E9%9A%9C/">调整 net.ipv4.tcp_tw_recycle 造成的故障</a></li>
<li><a href="https://www.iyunw.cn/archives/xie-lin-lin-jiao-xun-sheng-cheng-huan-jing-bu-yao-luan-xiu-gai-net-ipv4-tcp-tw-recycle/">血淋淋教训，生产环境不要乱修改net.ipv4.tcp_tw_recycle</a></li>
</ul>

<p>与<code>TIME_WAIT</code>对应的是<code>COSE_WAIT</code>，被动关闭方收到关闭请求后置为<code>CLOSE_WAIT</code>状态，但没有在主动发包，通常是程序里对资源没有调用<code>Close Socket</code>操作。</p>

<h3 id="net-ipv4-tcp-tw-reuse">net.ipv4.tcp_tw_reuse</h3>

<pre><code>net.ipv4.tcp_tw_reuse = 1
net.ipv4.tcp_timestamps = 1
</code></pre>

<p>开启<code>TCP</code>连接重用，允许将<code>TIME_WAIT</code>的连接用于新的TCP连接，默认为0，表示关闭。</p>

<h3 id="net-ipv4-tcp-fin-timeout">net.ipv4.tcp_fin_timeout</h3>

<pre><code>net.ipv4.tcp_fin_timeout = 30
</code></pre>

<p>设置<code>FIN-WAIT-2</code>超时时间，当主动关闭的一方处于该状态，但一直收不到对方的<code>FIN</code>包时，超过设定的时间就直接<code>CLOSE</code>掉了。</p>

<h3 id="net-ipv4-tcp-keepalive-time">net.ipv4.tcp_keepalive_time</h3>

<pre><code>net.ipv4.tcp_keepalive_time = 1200
net.ipv4.tcp_keepalive_intvl = 2
net.ipv4.tcp_keepalive_probes = 2
</code></pre>

<p>默认是2个小时。<code>tcp_keepalive_intvl</code>表示探测包发送的时间间隔，默认75s。<code>tcp_keepalive_probes</code>对方不予应答发送探测包的次数，默认是9次。</p>

<h3 id="net-ipv4-ip-local-reserved-ports">net.ipv4.ip_local_reserved_ports</h3>

<pre><code>net.ipv4.ip_local_port_range = 1024 65000
net.ipv4.ip_local_reserved_ports = 9001,30001
</code></pre>

<p>有时候重启服务的时候发现某个端口被占用导致服务无法启动，可以设置此参数，<code>TCP</code>建立连接时从<code>ip_local_port_range</code>中选取端口，同时会排除<code>ip_local_reserved_ports中</code>设定的端口。</p>

<h2 id="2-2-系统丢包">2.2 系统丢包</h2>

<p>当请求量到一定数量时可能会出现丢包的情况，这个时候就可能跟nf_conntrack模块的设置有关了。通过系统日志可以看到类似<code>kernel: nf_conntrack: table full, dropping packet</code>的日志信息。nf_conntrack模块会使用一个哈希表记录TCP协议“established connection”记录，当这个哈希表满之后，新的连接会引发“nf_conntrack: table full, dropping packet”错误。</p>

<h3 id="net-netfilter-nf-conntrack-max">net.netfilter.nf_conntrack_max</h3>

<pre><code>sudo sysctl -w net.netfilter.nf_conntrack_max=1503232
sudo sysctl -w net.netfilter.nf_conntrack_buckets=375808  # 如果使用非4.19内核，该选项可能无法在运行时修改
sudo sysctl -w net.netfilter.nf_conntrack_tcp_timeout_time_wait=60
</code></pre>

<h2 id="2-3-swap分区">2.3 Swap分区</h2>

<pre><code>vm.swappiness = 0
</code></pre>

<p>内核参数<code>vm.swappiness</code>控制换出运行时内存的相对权重，参数值大小对如何使用swap分区有很大联系。值越大，表示越积极使用swap分区，越小表示越积极使用物理内存。默认值swappiness=60，表示内存使用率超过100-60=40%时开始使用交换分区。</p>

<p><code>swappiness=0</code>的时候表示最大限度使用物理内存，然后才是 swap空间；swappiness＝100的时候表示积极使用swap分区，并把内存上的数据及时搬运到swap空间。</p>

<p>（网上有的说，对于3.5以后的内核和RedHat 2.6.32之后的内核，设置为0会禁止使用swap，从而引发out of memory，这种情况可以设置为1。）</p>

<p><strong>如果需要，可打开，然后设置Swap分区</strong>：</p>

<pre><code>dd if=/dev/zero of=/mnt/swap bs=block_size count=number_of_block
注：block_size、number_of_block 大小可以自定义，比如bs=1M count=1024 代表设置1G大小swap分区

$ dd if=/dev/zero of=/data/swap bs=10M count=2048
记录了2048+0 的读入
记录了2048+0 的写出
21474836480字节(21 GB)已复制，13.5878 秒，1.6 GB/秒

$ mkswap /mnt/swap
$ swapon /mnt/swap
$ echo &quot;/mnt/swap swap swap defaults 0 0&quot; &gt;&gt; /etc/fstab
$ free -m
             total       used       free     shared    buffers     cached
Mem:        127162      63028      64133          1        142      51670
-/+ buffers/cache:      11215     115946
Swap:        20479          0      20479
</code></pre>

<h2 id="2-4-内存分配策略">2.4 内存分配策略</h2>

<pre><code>vm.overcommit_memory = 1
</code></pre>

<p>它是 内存分配策略，可选值：0、1、2。</p>

<ul>
<li>0， 表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存，内存申请允许；否则，内存申请失败，并把错误返回给应用进程。</li>
<li>1， 表示内核允许分配所有的物理内存，而不管当前的内存状态如何。</li>
<li>2， 表示内核允许分配超过所有物理内存和交换空间总和的内存。</li>
</ul>

<h1 id="完整示例">完整示例</h1>

<pre><code>vm.swappiness = 0
kernel.sysrq=1

net.ipv4.neigh.default.gc_stale_time = 120

# see details in https://help.aliyun.com/knowledge_detail/39428.html
net.ipv4.conf.all.rp_filter = 0
net.ipv4.conf.default.rp_filter = 0
net.ipv4.conf.default.arp_announce = 2
net.ipv4.conf.lo.arp_announce = 2
net.ipv4.conf.all.arp_announce = 2

# see details in https://help.aliyun.com/knowledge_detail/41334.html
net.ipv4.tcp_max_tw_buckets = 20000
net.ipv4.tcp_syncookies = 1
net.ipv4.tcp_max_syn_backlog = 8192
net.ipv4.tcp_synack_retries = 2

net.ipv4.tcp_tw_reuse = 1 
net.ipv4.tcp_fin_timeout = 30
net.ipv4.tcp_keepalive_time = 1200

net.core.somaxconn = 65535
net.netfilter.nf_conntrack_max = 1503232
#net.netfilter.nf_conntrack_buckets = 375808
net.netfilter.nf_conntrack_tcp_timeout_established = 1200
</code></pre>

<p>Redis开启bgsave可考虑设置<code>vm.overcommit_memory = 1</code></p>

<hr />

<ul>
<li><a href="https://segmentfault.com/a/1190000019292140">深入浅出TCP中的SYN-Cookies</a></li>
<li><a href="http://keithmo.me/post/2018/08/25/conntrack-tuning/">[踩坑总结] nf_conntrack: table full, dropping packet</a></li>
</ul>

    <div class="eof">-- EOF --</div>
    <div class="eof_arrow">
        <a href="/"><img src="/static/img/arrow-back.png" style="width:25px;height:25px;" /></a>
    </div>
    
    <div class="eof_tag">
        最后更新于：
        <code style="border:0px;background:none;"><a href="/2017-12.html">2021-09-15 08:27</a></code>
    </div>
    
    <div class="eof_tag">
        发表于：
        <code style="border:0px;background:none;"><a href="/2017-12.html">2017-12-24 19:30</a></code>
    </div>
    <div class="eof_tag">
        标签：
        <code style="border:0px;background:none;"><a href="/tag/运维.html">运维</a></code>
        <code style="border:0px;background:none;"><a href="/tag/centos.html">CentOS</a></code>
    </div>

    <div id="footer">
        <ul>
            <li>
            <b>上一篇</b>：<a href="/linux-monitor.html">CentOS（ 4 ）- CPU、内存、端口等指标查看</a>
            </li>
            
            <li>
            <b>下一篇</b>：<a href="/lets-encrypt.html">Let's Encrypt配置SSL免费证书</a>
            </li>
            <li>
                <b>Github地址</b>：<a href="https://github.com/pengbotao/itopic.go/blob/master/posts\运维\CentOS（ 5 ）- 内核参数优化.md">https://github.com/pengbotao/itopic.go/blob/master/posts\运维\CentOS（ 5 ）- 内核参数优化.md</a>
            <li>
            <li>
                @2013-2021 老彭的博客&nbsp;[Hosted by <a href="https://pages.github.com/" style="font-weight: bold" target="_blank">Github Pages</a>]
            </li>
        </ul>
    </div>
</div>
<div id="top"><a href="#"><img src="/static/img/arrow-top.png" style="width:40px;height:40px;" /></a></div>

<a href="https://github.com/pengbotao/itopic.go/blob/master/posts\运维\CentOS（ 5 ）- 内核参数优化.md" target="_blank"  class="github-corner">
<svg width="60" height="60" viewBox="0 0 250 250" style="fill: #61687C; color:#fff; position: absolute;top: 0;border: 0;right: 0;" aria-hidden="true">
    <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
    <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
    <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path>
    </svg>
</a>
</body>
</html>
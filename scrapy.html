<!DOCTYPE html>
<html lang="zh-CN">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no">
    <title>Python爬虫库 ( 6 ) - Scrapy</title>
    <link rel="stylesheet" href="/static/css/markdown.css">
    
</head>
<body>
<div class="content">
    <h1 class="title">Python爬虫库 ( 6 ) - Scrapy</h1>

    <a href="/"><img src="/static/img/arrow-back.png" class="title_arrow_back" /></a>

    <h1 id="一-关于scrapy">一、关于Scrapy</h1>

<h2 id="1-1-文档地址">1.1 文档地址</h2>

<pre><code>https://doc.scrapy.org/en/latest/
</code></pre>

<h2 id="1-2-安装方式">1.2 安装方式</h2>

<pre><code>pip install scrapy
</code></pre>

<h1 id="二-抓取示例">二、抓取示例</h1>

<p>装好之后先进行一次<code>Scrapy</code>使用初体验，从开发过程中熟悉<code>Scrapy</code>的流程及用法。</p>

<h2 id="2-1-初始化">2.1 初始化</h2>

<pre><code>$ scrapy startproject demo
New Scrapy project 'demo', using template directory '...', created in:
    /Users/peng/workspace/demo

You can start your first spider with:
    cd demo
    scrapy genspider example example.com


$ cd demo/
$ scrapy genspider itopic itopic.org
Created spider 'itopic' using template 'basic' in module:
  demo.spiders.itopic
</code></pre>

<p><img src="../../static/uploads/scrapy-project-init.png" alt="" /></p>

<h2 id="2-2-启动抓取">2.2 启动抓取</h2>

<pre><code>scrapy crawl itopic
</code></pre>

<p>当然在这一步执行抓取除了可以看到一些日志之外，并不会得到其他的东西。目前抓取的需求还没有明确，还需要对<code>itopic.org</code>页面返回的数据进行解析。</p>

<h2 id="2-3-解析列表">2.3 解析列表</h2>

<p>解析文章和超链地址，在这一步之后若在次执行抓取，顺利的话在命令行就可以看到应有的内容了。</p>

<pre><code># -*- coding: utf-8 -*-
import scrapy
from lxml import html


class ItopicSpider(scrapy.Spider):
    name = 'itopic'
    allowed_domains = ['itopic.org']
    start_urls = ['https://itopic.org/']

    def parse(self, response):
        for x in html.fromstring(response.body).xpath('//div[@id=&quot;left-sider&quot; or @id=&quot;right-sider&quot;]/ul/li/a'):
            print(x.text, x.attrib.get('href'))
</code></pre>

<h2 id="2-4-解析详情">2.4 解析详情</h2>

<p>拿到首页的超链接之后通过<code>yield scrapy.Request(url=url, callback=self.parse_itopic_detail)</code>即可将详情页的请求发到调度器，同时使用<code>self.parse_itopic_detail</code>进行解析。所以对于新的页面只需要通过<code>yield scrapy.Request</code>添加地址，并定义对应的解析器即可。</p>

<pre><code># -*- coding: utf-8 -*-
import scrapy
from lxml import html


class ItopicSpider(scrapy.Spider):
    name = 'itopic'
    allowed_domains = ['itopic.org']
    start_urls = ['https://itopic.org/']

    def parse(self, response):
        for x in html.fromstring(response.body).xpath('//div[@id=&quot;left-sider&quot; or @id=&quot;right-sider&quot;]/ul/li/a'):
            url = &quot;https://itopic.org&quot; + x.attrib.get('href')
            yield scrapy.Request(url=url, callback=self.parse_itopic_detail)

    def parse_itopic_detail(self, response):
        filename = response.url.split(&quot;/&quot;)[-1]
        with open(filename, &quot;wb&quot;) as f:
            f.write(response.body)
        print(response.css(&quot;.title::text&quot;).extract()[0])
</code></pre>

<h2 id="2-5-保存结果">2.5 保存结果</h2>

<h3 id="2-5-1-编写item">2.5.1 编写Item</h3>

<pre><code># -*- coding: utf-8 -*-

# Define here the models for your scraped items
#
# See documentation in:
# https://docs.scrapy.org/en/latest/topics/items.html

import scrapy


class ItopicDetailItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    title = scrapy.Field()
    url = scrapy.Field()
    pass
</code></pre>

<h3 id="2-5-2-提取数据到item">2.5.2 提取数据到Item</h3>

<pre><code># -*- coding: utf-8 -*-
import scrapy
from demo.items import ItopicDetailItem
from lxml import html


class ItopicSpider(scrapy.Spider):
    name = 'itopic'
    allowed_domains = ['itopic.org']
    start_urls = ['https://itopic.org/']

    def parse(self, response):
        for x in html.fromstring(response.body).xpath('//div[@id=&quot;left-sider&quot; or @id=&quot;right-sider&quot;]/ul/li/a'):
            url = &quot;https://itopic.org&quot; + x.attrib.get('href')
            yield scrapy.Request(url=url, callback=self.parse_itopic_detail)

    def parse_itopic_detail(self, response):
        filename = response.url.split(&quot;/&quot;)[-1]
        with open(filename, &quot;wb&quot;) as f:
            f.write(response.body)
        item = ItopicDetailItem()
        item['title'] = response.css(&quot;.title::text&quot;).extract()[0]
        item['url'] = response.url
        yield item
</code></pre>

<h3 id="2-5-3-保存结果">2.5.3 保存结果</h3>

<p>执行抓取并保存结果到<code>itopic.json</code>文件。执行完成后查看<code>itopic.json</code>就可以看到抓取到的数据了。</p>

<pre><code>$ scrapy crawl itopic -o itopic.json

[
    {
        &quot;title&quot;: &quot;Python常用库 - HTTP请求&quot;,
        &quot;url&quot;: &quot;https://itopic.org/python-requests.html&quot;
    },
    {
        &quot;title&quot;: &quot;Python常用库 - 数据库&quot;,
        &quot;url&quot;: &quot;https://itopic.org/python-database.html&quot;
    },
    {
        &quot;title&quot;: &quot;Python常用库 - 数据解析&quot;,
        &quot;url&quot;: &quot;https://itopic.org/python-data-parse.html&quot;
    }
    ...
]
</code></pre>

<p>到这里一个简单的<code>Scrapy</code>抓取示例就算完成了，应该可以满足大部分抓取场景了。</p>

<h1 id="三-抓取说明">三、抓取说明</h1>

<h2 id="3-1-流程说明">3.1 流程说明</h2>

<p>一个常规的抓取流程应该包括以下几步骤：</p>

<ul>
<li>1. 从某一页面入口抓取解析出<code>URL</code>列表或者通过其他途径获取到需要抓取的<code>URL</code>列表。</li>
<li>2. 执行抓取动作，发送<code>Request</code>请求，获取返回的<code>HTML</code>内容</li>
<li>3. 通过<code>lxml</code>、<code>bs4</code>等解析页面元素，获取抓取内容</li>
<li>4. 将抓取到的内容持久化或者进行同步操作。</li>
</ul>

<p>这个过程中可能会碰到需要分页抓取所有的<code>URL</code>列表，相当于重复走一遍上面的流程，只不过抓取的目的是为了获取<code>URL</code>列表，拿到之后在重复此步骤。配合上面的示例来看看<code>Scrapy</code>的流程图：</p>

<p><img src="../../static/uploads/scrapy-engine.png" alt="" /></p>

<table>
<thead>
<tr>
<th>组件</th>
<th>说明</th>
</tr>
</thead>

<tbody>
<tr>
<td>Scrapy Engine</td>
<td>引擎负责控制数据流在系统中所有组件中流动，并在相应动作发生时触发事件。 详细内容查看下面的数据流(Data Flow)部分。</td>
</tr>

<tr>
<td>调度器(Scheduler)</td>
<td>调度器从引擎接受request并将他们入队，以便之后引擎请求他们时提供给引擎。</td>
</tr>

<tr>
<td>下载器(Downloader)</td>
<td>下载器负责获取页面数据并提供给引擎，而后提供给<code>spider</code>。</td>
</tr>

<tr>
<td>Spiders</td>
<td><code>Spider</code>是<code>Scrapy</code>用户编写用于分析response并提取item(即获取到的item)或额外跟进的URL的类。 每个<code>spider</code>负责处理一个特定(或一些)网站。</td>
</tr>

<tr>
<td>Item Pipeline</td>
<td><code>Item Pipeline</code>负责处理被spider提取出来的item。典型的处理有清理、 验证及持久化(例如存取到数据库中)。</td>
</tr>

<tr>
<td>下载器中间件(Downloader middlewares)</td>
<td>下载器中间件是在引擎及下载器之间的特定钩子(<code>specific hook</code>)，处理<code>Downloader</code>传递给引擎的response。 其提供了一个简便的机制，通过插入自定义代码来扩展<code>Scrapy</code>功能。</td>
</tr>

<tr>
<td>Spider中间件(Spider middlewares)</td>
<td><code>Spider</code>中间件是在引擎及<code>Spider</code>之间的特定钩子(<code>specific hook</code>)，处理<code>spider</code>的输入(response)和输出(items及requests)。 其提供了一个简便的机制，通过插入自定义代码来扩展<code>Scrapy</code>功能。</td>
</tr>
</tbody>
</table>

<p>可以看到分工更细更清晰，将要抓取的<code>URL</code>发给调度器，将<code>Request</code>的动作交给下载器，增加数据的清洗流程，同时以扩展中间件的方式实现灵活的流程控制。<code>Scrapy</code>将这些组件串起来，我们只需要关注核心的几个步骤即可。</p>

<h2 id="3-2-数据流-data-flow">3.2 数据流(Data flow)</h2>

<p><code>Scrapy</code>中的数据流由执行引擎控制，其过程如下:</p>

<ul>
<li>1. 引擎打开一个网站(open a domain)，找到处理该网站的<code>Spider</code>并向该<code>spider</code>请求第一个要爬取的<code>URL(s)</code>。</li>
<li>2. 引擎从<code>Spider</code>中获取到第一个要爬取的URL并在调度器(<code>Scheduler</code>)以<code>Request</code>调度。</li>
<li>3. 引擎向调度器请求下一个要爬取的<code>URL</code>。</li>
<li>4. 调度器返回下一个要爬取的<code>URL</code>给引擎，引擎将<code>URL</code>通过下载中间件(请求(<code>request</code>)方向)转发给下载器(<code>Downloader</code>)。</li>
<li>5. 一旦页面下载完毕，下载器生成一个该页面的<code>Response</code>，并将其通过下载中间件(返回(response)方向)发送给引擎。</li>
<li>6. 引擎从下载器中接收到<code>Response</code>并通过<code>Spider</code>中间件(输入方向)发送给Spider处理。</li>
<li>7. <code>Spider</code>处理Response并返回爬取到的Item及(跟进的)新的Request给引擎。</li>
<li>8. 引擎将(Spider返回的)爬取到的Item给<code>Item Pipeline</code>，将(Spider返回的)Request给调度器。</li>
<li>9. (从第二步)重复直到调度器中没有更多地request，引擎关闭该网站。</li>
</ul>

    <div class="eof">-- EOF --</div>
    <div class="eof_arrow">
        <a href="/"><img src="/static/img/arrow-back.png" style="width:25px;height:25px;" /></a>
    </div>
    
    <div class="eof_tag">
        最后更新于：
        <code style="border:0px;background:none;"><a href="/2017-05.html">2021-11-27 13:41</a></code>
    </div>
    
    <div class="eof_tag">
        发表于：
        <code style="border:0px;background:none;"><a href="/2017-05.html">2017-05-24 23:12</a></code>
    </div>
    <div class="eof_tag">
        标签：
        <code style="border:0px;background:none;"><a href="/tag/python.html">Python</a></code>
        <code style="border:0px;background:none;"><a href="/tag/爬虫.html">爬虫</a></code>
    </div>

    <div id="footer">
        <ul>
            <li>
            <b>上一篇</b>：<a href="/beautifulsoup.html">Python爬虫库 ( 5 ) - BeautifulSoup</a>
            </li>
            
            <li>
            <b>下一篇</b>：<a href="/order-holtwinter.html">外卖订单量预测异常报警模型实践</a>
            </li>
            <li>
                <b>Github地址</b>：<a href="https://github.com/pengbotao/itopic.go/blob/master/posts/python/Python爬虫库 ( 6 ) - Scrapy.md">https://github.com/pengbotao/itopic.go/blob/master/posts/python/Python爬虫库 ( 6 ) - Scrapy.md</a>
            <li>
            <li>
                @2013-2022 老彭的博客&nbsp;[Hosted by <a href="https://pages.github.com/" style="font-weight: bold" target="_blank">Github Pages</a>]
            </li>
        </ul>
    </div>
</div>
<div id="top"><a href="#"><img src="/static/img/arrow-top.png" style="width:40px;height:40px;" /></a></div>

<a href="https://github.com/pengbotao/itopic.go/blob/master/posts/python/Python爬虫库 ( 6 ) - Scrapy.md" target="_blank"  class="github-corner">
<svg width="60" height="60" viewBox="0 0 250 250" style="fill: #61687C; color:#fff; position: absolute;top: 0;border: 0;right: 0;" aria-hidden="true">
    <path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path>
    <path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path>
    <path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path>
    </svg>
</a>
</body>
</html>